"""
ğŸ˜Š GAIM Lab v3.0 - Emotion Detector
êµì‚¬/í•™ìƒ í‘œì • ë° ìŒì„± ê°ì • ë¶„ì„ ëª¨ë“ˆ

Features:
- DeepFace ê¸°ë°˜ í‘œì • ì¸ì‹ (7ê°€ì§€ ê°ì •)
- ìŒì„± ê°ì • ì¸ì‹ (SER)
- íƒ€ì„ë¼ì¸ ê¸°ë°˜ ê°ì • ì¶”ì 
- êµì‹¤ ë¶„ìœ„ê¸° ì¢…í•© ë¶„ì„
"""

import os
import numpy as np
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass, field
from collections import Counter
import time

# Image processing
try:
    from PIL import Image
    import cv2
    IMAGING_AVAILABLE = True
except ImportError:
    IMAGING_AVAILABLE = False

# DeepFace for facial emotion
try:
    from deepface import DeepFace
    DEEPFACE_AVAILABLE = True
except ImportError:
    DEEPFACE_AVAILABLE = False

# Audio processing
try:
    import librosa
    LIBROSA_AVAILABLE = True
except ImportError:
    LIBROSA_AVAILABLE = False


@dataclass
class EmotionFrame:
    """ë‹¨ì¼ í”„ë ˆì„/ì„¸ê·¸ë¨¼íŠ¸ ê°ì • ë°ì´í„°"""
    timestamp: float
    dominant_emotion: str
    emotion_scores: Dict[str, float]
    confidence: float = 0.0
    source: str = "face"  # "face" or "voice"


@dataclass
class EmotionTimeline:
    """ê°ì • íƒ€ì„ë¼ì¸ ë¶„ì„ ê²°ê³¼"""
    frames: List[EmotionFrame]
    summary: Dict[str, float]  # ê°ì •ë³„ ë¹„ìœ¨
    positive_ratio: float
    negative_ratio: float
    neutral_ratio: float
    dominant_emotion: str
    mood_transitions: List[Dict]  # ë¶„ìœ„ê¸° ë³€í™” ì§€ì 


# ê°ì • ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜
POSITIVE_EMOTIONS = ["happy", "surprise"]
NEGATIVE_EMOTIONS = ["sad", "angry", "fear", "disgust"]
NEUTRAL_EMOTIONS = ["neutral"]


class EmotionDetector:
    """
    ğŸ˜Š ê°ì • ë¶„ì„ê¸°
    
    í‘œì •ê³¼ ìŒì„±ì—ì„œ ê°ì •ì„ ì¶”ì¶œí•˜ì—¬ ìˆ˜ì—… ë¶„ìœ„ê¸°ë¥¼ ë¶„ì„
    """
    
    # ê°ì • ë¼ë²¨ (í•œê¸€)
    EMOTION_LABELS_KO = {
        "happy": "í–‰ë³µ ğŸ˜Š",
        "sad": "ìŠ¬í”” ğŸ˜¢",
        "angry": "ë¶„ë…¸ ğŸ˜ ",
        "fear": "ê³µí¬ ğŸ˜¨",
        "surprise": "ë†€ëŒ ğŸ˜²",
        "disgust": "í˜ì˜¤ ğŸ¤¢",
        "neutral": "ë¬´í‘œì • ğŸ˜"
    }
    
    def __init__(
        self,
        face_detector: str = "opencv",
        analyze_voice: bool = True,
        segment_duration: float = 10.0
    ):
        """
        Args:
            face_detector: ì–¼êµ´ ê°ì§€ ë°±ì—”ë“œ ("opencv", "retinaface", "mtcnn")
            analyze_voice: ìŒì„± ê°ì • ë¶„ì„ ì—¬ë¶€
            segment_duration: íƒ€ì„ë¼ì¸ ì„¸ê·¸ë¨¼íŠ¸ ê¸¸ì´ (ì´ˆ)
        """
        self.face_detector = face_detector
        self.analyze_voice = analyze_voice
        self.segment_duration = segment_duration
        
        self._check_dependencies()
        
        print(f"ğŸ˜Š [EmotionDetector] ì´ˆê¸°í™”: face_detector={face_detector}")
    
    def _check_dependencies(self):
        """ì˜ì¡´ì„± í™•ì¸"""
        if not IMAGING_AVAILABLE:
            print("   âš ï¸ PIL/OpenCV not available")
        if not DEEPFACE_AVAILABLE:
            print("   âš ï¸ DeepFace not available - facial emotion disabled")
        if not LIBROSA_AVAILABLE:
            print("   âš ï¸ librosa not available - voice emotion disabled")
    
    def analyze_frame(self, image_path: str) -> Optional[EmotionFrame]:
        """
        ë‹¨ì¼ ì´ë¯¸ì§€ì—ì„œ í‘œì • ë¶„ì„
        
        Args:
            image_path: ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œ
            
        Returns:
            EmotionFrame ë˜ëŠ” None (ì–¼êµ´ ë¯¸ê°ì§€ ì‹œ)
        """
        if not DEEPFACE_AVAILABLE:
            return None
        
        try:
            # Analyze with DeepFace
            result = DeepFace.analyze(
                img_path=image_path,
                actions=["emotion"],
                detector_backend=self.face_detector,
                enforce_detection=False,
                silent=True
            )
            
            if isinstance(result, list):
                result = result[0]
            
            emotion_scores = result.get("emotion", {})
            dominant = result.get("dominant_emotion", "neutral")
            
            # Normalize scores to 0-1
            total = sum(emotion_scores.values())
            if total > 0:
                emotion_scores = {k: v / total for k, v in emotion_scores.items()}
            
            return EmotionFrame(
                timestamp=0.0,  # Will be set by caller
                dominant_emotion=dominant,
                emotion_scores=emotion_scores,
                confidence=emotion_scores.get(dominant, 0),
                source="face"
            )
            
        except Exception as e:
            return None
    
    def analyze_frames_batch(
        self, 
        frame_paths: List[str],
        timestamps: Optional[List[float]] = None
    ) -> List[EmotionFrame]:
        """
        ì—¬ëŸ¬ í”„ë ˆì„ ë°°ì¹˜ ë¶„ì„
        
        Args:
            frame_paths: í”„ë ˆì„ ì´ë¯¸ì§€ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
            timestamps: ê° í”„ë ˆì„ì˜ íƒ€ì„ìŠ¤íƒ¬í”„ (ì—†ìœ¼ë©´ ì¸ë±ìŠ¤ ì‚¬ìš©)
            
        Returns:
            EmotionFrame ë¦¬ìŠ¤íŠ¸
        """
        results = []
        
        for i, path in enumerate(frame_paths):
            frame = self.analyze_frame(path)
            
            if frame:
                frame.timestamp = timestamps[i] if timestamps else float(i)
                results.append(frame)
        
        return results
    
    def analyze_audio_emotion(
        self,
        audio_path: str,
        segment_duration: float = None
    ) -> List[EmotionFrame]:
        """
        ì˜¤ë””ì˜¤ì—ì„œ ìŒì„± ê°ì • ë¶„ì„
        
        ê°„ë‹¨í•œ íŠ¹ì„± ê¸°ë°˜ ê°ì • ì¶”ë¡  (ML ëª¨ë¸ ì—†ì´)
        - ì—ë„ˆì§€ ë†’ìŒ + í”¼ì¹˜ ë†’ìŒ â†’ happy/surprise
        - ì—ë„ˆì§€ ë‚®ìŒ â†’ sad/neutral
        - ì—ë„ˆì§€ ë³€ë™ í¼ â†’ angry
        
        Args:
            audio_path: ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
            segment_duration: ì„¸ê·¸ë¨¼íŠ¸ ê¸¸ì´
            
        Returns:
            EmotionFrame ë¦¬ìŠ¤íŠ¸
        """
        if not LIBROSA_AVAILABLE:
            return []
        
        segment_duration = segment_duration or self.segment_duration
        
        try:
            # Load audio
            y, sr = librosa.load(audio_path, sr=16000)
            duration = len(y) / sr
            
            results = []
            n_segments = int(np.ceil(duration / segment_duration))
            
            for i in range(n_segments):
                start = int(i * segment_duration * sr)
                end = int(min((i + 1) * segment_duration * sr, len(y)))
                segment = y[start:end]
                
                if len(segment) < sr * 0.5:  # Skip very short segments
                    continue
                
                # Extract features
                energy = np.sqrt(np.mean(segment ** 2))
                energy_var = np.var(np.abs(segment))
                
                # Zero crossing rate (ê¸´ë°•í•¨ ì§€í‘œ)
                zcr = np.mean(librosa.feature.zero_crossing_rate(segment))
                
                # Spectral centroid (ë°ê¸°)
                cent = np.mean(librosa.feature.spectral_centroid(y=segment, sr=sr))
                
                # Infer emotion from features
                emotion = self._infer_emotion_from_features(energy, energy_var, zcr, cent)
                
                results.append(EmotionFrame(
                    timestamp=i * segment_duration,
                    dominant_emotion=emotion["dominant"],
                    emotion_scores=emotion["scores"],
                    confidence=emotion["confidence"],
                    source="voice"
                ))
            
            return results
            
        except Exception as e:
            print(f"   âš ï¸ Audio emotion analysis error: {e}")
            return []
    
    def _infer_emotion_from_features(
        self,
        energy: float,
        energy_var: float,
        zcr: float,
        centroid: float
    ) -> Dict:
        """íŠ¹ì„± ê¸°ë°˜ ê°ì • ì¶”ë¡ """
        
        # Normalize features to 0-1 range (approximate)
        energy_norm = min(energy * 10, 1.0)
        var_norm = min(energy_var * 100, 1.0)
        zcr_norm = min(zcr * 5, 1.0)
        cent_norm = min(centroid / 4000, 1.0)
        
        # Simple rule-based inference
        scores = {
            "happy": 0.0,
            "sad": 0.0,
            "angry": 0.0,
            "fear": 0.0,
            "surprise": 0.0,
            "disgust": 0.0,
            "neutral": 0.3
        }
        
        # High energy + high pitch â†’ happy/excited
        if energy_norm > 0.6 and cent_norm > 0.5:
            scores["happy"] = 0.5 + energy_norm * 0.3
            scores["surprise"] = 0.3
        
        # Low energy â†’ sad
        elif energy_norm < 0.3:
            scores["sad"] = 0.4 + (1 - energy_norm) * 0.3
            scores["neutral"] = 0.3
        
        # High variance â†’ angry/excited
        elif var_norm > 0.5:
            scores["angry"] = 0.4 + var_norm * 0.3
        
        # High ZCR â†’ nervous
        elif zcr_norm > 0.6:
            scores["fear"] = 0.3
            scores["surprise"] = 0.3
        
        else:
            scores["neutral"] = 0.6
        
        # Normalize
        total = sum(scores.values())
        scores = {k: v / total for k, v in scores.items()}
        
        dominant = max(scores, key=scores.get)
        
        return {
            "dominant": dominant,
            "scores": scores,
            "confidence": scores[dominant]
        }
    
    def build_timeline(
        self,
        face_frames: List[EmotionFrame],
        voice_frames: Optional[List[EmotionFrame]] = None
    ) -> EmotionTimeline:
        """
        ê°ì • íƒ€ì„ë¼ì¸ êµ¬ì¶•
        
        Args:
            face_frames: í‘œì • ê¸°ë°˜ ê°ì •
            voice_frames: ìŒì„± ê¸°ë°˜ ê°ì • (ì„ íƒ)
            
        Returns:
            EmotionTimeline ê°ì²´
        """
        all_frames = list(face_frames)
        if voice_frames:
            all_frames.extend(voice_frames)
        
        # Sort by timestamp
        all_frames.sort(key=lambda x: x.timestamp)
        
        if not all_frames:
            return EmotionTimeline(
                frames=[],
                summary={},
                positive_ratio=0.0,
                negative_ratio=0.0,
                neutral_ratio=0.0,
                dominant_emotion="neutral",
                mood_transitions=[]
            )
        
        # Calculate emotion distribution
        emotions = [f.dominant_emotion for f in all_frames]
        emotion_counts = Counter(emotions)
        total = len(emotions)
        
        summary = {k: v / total for k, v in emotion_counts.items()}
        
        # Calculate sentiment ratios
        positive_ratio = sum(summary.get(e, 0) for e in POSITIVE_EMOTIONS)
        negative_ratio = sum(summary.get(e, 0) for e in NEGATIVE_EMOTIONS)
        neutral_ratio = sum(summary.get(e, 0) for e in NEUTRAL_EMOTIONS)
        
        # Find dominant
        dominant = emotion_counts.most_common(1)[0][0]
        
        # Detect mood transitions
        transitions = self._detect_transitions(all_frames)
        
        return EmotionTimeline(
            frames=all_frames,
            summary=summary,
            positive_ratio=positive_ratio,
            negative_ratio=negative_ratio,
            neutral_ratio=neutral_ratio,
            dominant_emotion=dominant,
            mood_transitions=transitions
        )
    
    def _detect_transitions(
        self,
        frames: List[EmotionFrame],
        min_duration: float = 5.0
    ) -> List[Dict]:
        """ë¶„ìœ„ê¸° ë³€í™” ì§€ì  ê°ì§€"""
        transitions = []
        
        if len(frames) < 2:
            return transitions
        
        prev_emotion = frames[0].dominant_emotion
        prev_category = self._get_category(prev_emotion)
        segment_start = frames[0].timestamp
        
        for frame in frames[1:]:
            curr_category = self._get_category(frame.dominant_emotion)
            
            if curr_category != prev_category:
                duration = frame.timestamp - segment_start
                if duration >= min_duration:
                    transitions.append({
                        "timestamp": frame.timestamp,
                        "from_mood": prev_category,
                        "to_mood": curr_category,
                        "from_emotion": prev_emotion,
                        "to_emotion": frame.dominant_emotion
                    })
                
                prev_category = curr_category
                prev_emotion = frame.dominant_emotion
                segment_start = frame.timestamp
        
        return transitions
    
    def _get_category(self, emotion: str) -> str:
        """ê°ì • ì¹´í…Œê³ ë¦¬ ë°˜í™˜"""
        if emotion in POSITIVE_EMOTIONS:
            return "positive"
        elif emotion in NEGATIVE_EMOTIONS:
            return "negative"
        return "neutral"
    
    def analyze_classroom_mood(
        self,
        frame_dir: str,
        audio_path: Optional[str] = None
    ) -> Dict:
        """
        êµì‹¤ ì „ì²´ ë¶„ìœ„ê¸° ì¢…í•© ë¶„ì„
        
        Args:
            frame_dir: í”„ë ˆì„ ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬
            audio_path: ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ (ì„ íƒ)
            
        Returns:
            ì¢…í•© ë¶„ì„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        frame_dir = Path(frame_dir)
        
        # Get all frames
        frame_paths = sorted(frame_dir.glob("*.jpg")) + sorted(frame_dir.glob("*.png"))
        timestamps = [float(i) for i in range(len(frame_paths))]
        
        print(f"ğŸ˜Š [EmotionDetector] ë¶„ì„ ì‹œì‘: {len(frame_paths)} í”„ë ˆì„")
        
        # Analyze faces
        start_time = time.time()
        face_frames = self.analyze_frames_batch(
            [str(p) for p in frame_paths],
            timestamps
        )
        
        # Analyze voice if available
        voice_frames = []
        if audio_path and self.analyze_voice:
            voice_frames = self.analyze_audio_emotion(audio_path)
        
        # Build timeline
        timeline = self.build_timeline(face_frames, voice_frames)
        
        elapsed = time.time() - start_time
        
        result = {
            "timeline": timeline,
            "summary": {
                "dominant_emotion": timeline.dominant_emotion,
                "dominant_emotion_ko": self.EMOTION_LABELS_KO.get(
                    timeline.dominant_emotion, timeline.dominant_emotion
                ),
                "positive_ratio": timeline.positive_ratio,
                "negative_ratio": timeline.negative_ratio,
                "neutral_ratio": timeline.neutral_ratio,
                "emotion_distribution": timeline.summary,
                "mood_transitions_count": len(timeline.mood_transitions),
                "frames_analyzed": len(face_frames),
                "processing_time": elapsed
            },
            "recommendations": self._generate_recommendations(timeline)
        }
        
        print(f"   âœ… ë¶„ì„ ì™„ë£Œ: ì£¼ìš” ê°ì •={timeline.dominant_emotion}, "
              f"ê¸ì •={timeline.positive_ratio:.1%}, "
              f"ë¶€ì •={timeline.negative_ratio:.1%}")
        
        return result
    
    def _generate_recommendations(self, timeline: EmotionTimeline) -> List[str]:
        """ê°ì • ë¶„ì„ ê¸°ë°˜ ê°œì„  ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        if timeline.negative_ratio > 0.3:
            recommendations.append(
                "ğŸ˜Ÿ ë¶€ì •ì  ê°ì • ë¹„ìœ¨ì´ ë†’ìŠµë‹ˆë‹¤. ë” ë°ê³  í™œê¸°ì°¬ í‘œí˜„ì„ ì—°ìŠµí•´ ë³´ì„¸ìš”."
            )
        
        if timeline.neutral_ratio > 0.6:
            recommendations.append(
                "ğŸ˜ ë¬´í‘œì •ì´ ë§ìŠµë‹ˆë‹¤. í•™ìƒë“¤ê³¼ì˜ ìƒí˜¸ì‘ìš©ì—ì„œ ë” ë‹¤ì–‘í•œ ê°ì •ì„ í‘œí˜„í•´ ë³´ì„¸ìš”."
            )
        
        if timeline.positive_ratio > 0.5:
            recommendations.append(
                "ğŸ˜Š ê¸ì •ì ì¸ ë¶„ìœ„ê¸°ê°€ ì˜ ìœ ì§€ë˜ê³  ìˆìŠµë‹ˆë‹¤. í›Œë¥­í•©ë‹ˆë‹¤!"
            )
        
        if len(timeline.mood_transitions) > 5:
            recommendations.append(
                "ğŸ­ ê°ì • ë³€í™”ê°€ ë¹ˆë²ˆí•©ë‹ˆë‹¤. ì¼ê´€ëœ ë¶„ìœ„ê¸° ìœ ì§€ë¥¼ ì—°ìŠµí•´ ë³´ì„¸ìš”."
            )
        
        return recommendations


# CLI test
if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1:
        path = Path(sys.argv[1])
        
        detector = EmotionDetector()
        
        if path.is_dir():
            result = detector.analyze_classroom_mood(str(path))
            print(f"\nğŸ“Š ë¶„ì„ ê²°ê³¼:")
            print(f"   ì£¼ìš” ê°ì •: {result['summary']['dominant_emotion_ko']}")
            print(f"   ê¸ì •: {result['summary']['positive_ratio']:.1%}")
            print(f"   ë¶€ì •: {result['summary']['negative_ratio']:.1%}")
            print(f"   ì¤‘ë¦½: {result['summary']['neutral_ratio']:.1%}")
            print(f"\nğŸ’¡ ê¶Œì¥ì‚¬í•­:")
            for rec in result['recommendations']:
                print(f"   {rec}")
        else:
            frame = detector.analyze_frame(str(path))
            if frame:
                print(f"ê°ì •: {frame.dominant_emotion}")
                print(f"ì ìˆ˜: {frame.emotion_scores}")
    else:
        print("Usage: python emotion_detector.py <frame_dir_or_image>")
