"""
âš¡ GAIM Lab v3.0 - Faster-Whisper STT
Whisper ëŒ€ë¹„ 3ë°° ë¹ ë¥¸ ìŒì„± ì¸ì‹ ì—”ì§„

Features:
- CUDA ê°€ì† ì§€ì›
- ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ
- í•œêµ­ì–´ ìµœì í™”
- ì„¸ê·¸ë¨¼íŠ¸ë³„ íƒ€ì„ìŠ¤íƒ¬í”„
"""

import os
import subprocess
import tempfile
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass, field
import time

# Try faster-whisper first, fallback to whisper
try:
    from faster_whisper import WhisperModel
    FASTER_WHISPER_AVAILABLE = True
except ImportError:
    FASTER_WHISPER_AVAILABLE = False

try:
    import whisper
    WHISPER_AVAILABLE = True
except ImportError:
    WHISPER_AVAILABLE = False


@dataclass
class TranscriptSegment:
    """ìŒì„± ì¸ì‹ ì„¸ê·¸ë¨¼íŠ¸"""
    start: float
    end: float
    text: str
    confidence: float = 1.0
    words: List[Dict] = field(default_factory=list)


@dataclass
class TranscriptionResult:
    """ì „ì‚¬ ê²°ê³¼"""
    text: str
    segments: List[TranscriptSegment]
    language: str
    duration: float
    processing_time: float
    filler_words: Dict[str, int] = field(default_factory=dict)


class FasterWhisperSTT:
    """
    âš¡ ì´ˆê³ ì† ìŒì„± ì¸ì‹ ì—”ì§„
    
    Faster-Whisperë¥¼ ì‚¬ìš©í•˜ì—¬ Whisper ëŒ€ë¹„ 3ë°° ë¹ ë¥¸ STT ìˆ˜í–‰
    """
    
    # Korean filler words to detect
    FILLER_WORDS_KO = [
        "ìŒ", "ì–´", "ê·¸", "ì´ì œ", "ë­", "ê·¸ë˜ì„œ", "ì•„", "ì—", 
        "ê·¸ëŸ¬ë‹ˆê¹Œ", "ì¼ë‹¨", "ë§‰", "ì¢€", "ê·¼ë°", "ì €ê¸°"
    ]
    
    def __init__(
        self,
        model_size: str = "small",
        language: str = "ko",
        compute_type: str = "float16",
        device: str = "auto"
    ):
        """
        Args:
            model_size: ëª¨ë¸ í¬ê¸° ("tiny", "base", "small", "medium", "large")
            language: ì–¸ì–´ ì½”ë“œ ("ko", "en" ë“±)
            compute_type: ì—°ì‚° íƒ€ì… ("float16", "int8", "float32")
            device: ì¥ì¹˜ ("cuda", "cpu", "auto")
        """
        self.model_size = model_size
        self.language = language
        self.compute_type = compute_type
        self.device = self._detect_device() if device == "auto" else device
        
        self._model = None
        self._fallback_model = None
        
        print(f"ğŸ¤ [FasterWhisperSTT] ì´ˆê¸°í™”: model={model_size}, lang={language}, device={self.device}")
    
    def _detect_device(self) -> str:
        """CUDA ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸"""
        try:
            import torch
            return "cuda" if torch.cuda.is_available() else "cpu"
        except ImportError:
            return "cpu"
    
    def _load_model(self):
        """ëª¨ë¸ ë¡œë“œ (ì§€ì—° ë¡œë”©)"""
        if self._model is not None:
            return
        
        if FASTER_WHISPER_AVAILABLE:
            print(f"   ğŸ“¦ Faster-Whisper ëª¨ë¸ ë¡œë”©: {self.model_size}")
            self._model = WhisperModel(
                self.model_size,
                device=self.device,
                compute_type=self.compute_type
            )
        elif WHISPER_AVAILABLE:
            print(f"   ğŸ“¦ OpenAI Whisper ëª¨ë¸ ë¡œë”© (fallback): {self.model_size}")
            self._fallback_model = whisper.load_model(self.model_size)
        else:
            raise ImportError("faster-whisper ë˜ëŠ” whisper ì¤‘ í•˜ë‚˜ê°€ í•„ìš”í•©ë‹ˆë‹¤")
    
    def transcribe_audio(self, audio_path: str) -> TranscriptionResult:
        """
        ì˜¤ë””ì˜¤ íŒŒì¼ ì „ì‚¬
        
        Args:
            audio_path: ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ (WAV ê¶Œì¥)
            
        Returns:
            TranscriptionResult ê°ì²´
        """
        start_time = time.time()
        audio_path = Path(audio_path)
        
        if not audio_path.exists():
            raise FileNotFoundError(f"Audio file not found: {audio_path}")
        
        self._load_model()
        
        if FASTER_WHISPER_AVAILABLE and self._model:
            result = self._transcribe_faster(audio_path)
        else:
            result = self._transcribe_whisper(audio_path)
        
        result.processing_time = time.time() - start_time
        result.filler_words = self._detect_filler_words(result.text)
        
        print(f"   âœ… STT ì™„ë£Œ: {len(result.text)}ì, {len(result.segments)}ê°œ ì„¸ê·¸ë¨¼íŠ¸, {result.processing_time:.1f}ì´ˆ")
        
        return result
    
    def _transcribe_faster(self, audio_path: Path) -> TranscriptionResult:
        """Faster-Whisperë¡œ ì „ì‚¬"""
        segments_iter, info = self._model.transcribe(
            str(audio_path),
            language=self.language,
            word_timestamps=True,
            vad_filter=True,  # Voice activity detection
            vad_parameters=dict(
                min_silence_duration_ms=500,
                speech_pad_ms=200
            )
        )
        
        segments = []
        full_text = []
        
        for seg in segments_iter:
            segment = TranscriptSegment(
                start=seg.start,
                end=seg.end,
                text=seg.text.strip(),
                confidence=seg.avg_logprob if hasattr(seg, 'avg_logprob') else 1.0
            )
            
            # Extract word-level timestamps if available
            if hasattr(seg, 'words') and seg.words:
                segment.words = [
                    {"word": w.word, "start": w.start, "end": w.end}
                    for w in seg.words
                ]
            
            segments.append(segment)
            full_text.append(seg.text.strip())
        
        return TranscriptionResult(
            text=" ".join(full_text),
            segments=segments,
            language=info.language,
            duration=info.duration,
            processing_time=0.0  # Will be set later
        )
    
    def _transcribe_whisper(self, audio_path: Path) -> TranscriptionResult:
        """OpenAI Whisperë¡œ ì „ì‚¬ (í´ë°±)"""
        result = self._fallback_model.transcribe(
            str(audio_path),
            language=self.language
        )
        
        segments = []
        for seg in result.get("segments", []):
            segments.append(TranscriptSegment(
                start=seg["start"],
                end=seg["end"],
                text=seg["text"].strip()
            ))
        
        # Calculate duration from last segment
        duration = segments[-1].end if segments else 0.0
        
        return TranscriptionResult(
            text=result.get("text", ""),
            segments=segments,
            language=self.language,
            duration=duration,
            processing_time=0.0
        )
    
    def _detect_filler_words(self, text: str) -> Dict[str, int]:
        """ìŠµê´€ì–´(êµ°ë”ë”ê¸° ë§) ê°ì§€"""
        filler_counts = {}
        
        # Simple word-based detection
        words = text.split()
        
        for filler in self.FILLER_WORDS_KO:
            count = sum(1 for w in words if filler in w)
            if count > 0:
                filler_counts[filler] = count
        
        return filler_counts
    
    def transcribe_video(self, video_path: str) -> TranscriptionResult:
        """
        ë¹„ë””ì˜¤ íŒŒì¼ì—ì„œ ì˜¤ë””ì˜¤ ì¶”ì¶œ í›„ ì „ì‚¬
        
        Args:
            video_path: ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
            
        Returns:
            TranscriptionResult ê°ì²´
        """
        video_path = Path(video_path)
        
        if not video_path.exists():
            raise FileNotFoundError(f"Video file not found: {video_path}")
        
        # Extract audio to temp file
        with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as temp_audio:
            temp_audio_path = temp_audio.name
        
        try:
            print(f"   ğŸ¬ ì˜ìƒì—ì„œ ì˜¤ë””ì˜¤ ì¶”ì¶œ ì¤‘...")
            cmd = [
                "ffmpeg", "-y",
                "-i", str(video_path),
                "-ar", "16000",
                "-ac", "1",
                "-acodec", "pcm_s16le",
                temp_audio_path,
                "-loglevel", "error"
            ]
            subprocess.run(cmd, check=True)
            
            return self.transcribe_audio(temp_audio_path)
            
        finally:
            # Cleanup temp file
            if os.path.exists(temp_audio_path):
                os.remove(temp_audio_path)
    
    def get_speech_segments_with_timestamps(
        self, 
        audio_path: str
    ) -> List[Dict]:
        """
        ë°œí™” êµ¬ê°„ê³¼ íƒ€ì„ìŠ¤íƒ¬í”„ ë°˜í™˜ (íƒ€ì„ë¼ì¸ ë§ˆì»¤ìš©)
        
        Returns:
            [{"start": 0.0, "end": 5.2, "text": "...", "type": "speech"}, ...]
        """
        result = self.transcribe_audio(audio_path)
        
        markers = []
        for seg in result.segments:
            markers.append({
                "start": seg.start,
                "end": seg.end,
                "text": seg.text,
                "type": "speech",
                "confidence": seg.confidence
            })
            
            # Mark filler words
            for filler in self.FILLER_WORDS_KO:
                if filler in seg.text:
                    markers.append({
                        "start": seg.start,
                        "end": seg.end,
                        "text": f"ìŠµê´€ì–´ ê°ì§€: '{filler}'",
                        "type": "filler_word"
                    })
        
        return markers


# CLI test
if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1:
        audio_or_video = Path(sys.argv[1])
        
        stt = FasterWhisperSTT(model_size="small", language="ko")
        
        if audio_or_video.suffix.lower() in [".mp4", ".avi", ".mov", ".mkv"]:
            result = stt.transcribe_video(str(audio_or_video))
        else:
            result = stt.transcribe_audio(str(audio_or_video))
        
        print(f"\nğŸ“ ì „ì‚¬ ê²°ê³¼:")
        print(f"   í…ìŠ¤íŠ¸ ê¸¸ì´: {len(result.text)}ì")
        print(f"   ì„¸ê·¸ë¨¼íŠ¸ ìˆ˜: {len(result.segments)}")
        print(f"   ì²˜ë¦¬ ì‹œê°„: {result.processing_time:.1f}ì´ˆ")
        print(f"   ìŠµê´€ì–´: {result.filler_words}")
        print(f"\n--- ì²˜ìŒ 500ì ---")
        print(result.text[:500])
    else:
        print("Usage: python faster_whisper_stt.py <audio_or_video_path>")
